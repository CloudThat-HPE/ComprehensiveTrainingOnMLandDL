{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3402ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"titanic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2dc9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To check null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7e7a4",
   "metadata": {},
   "source": [
    "The info() function is used to print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa48c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d90cdc",
   "metadata": {},
   "source": [
    "The describe() method is used for calculating some statistical data like percentile, mean and std of the numerical values of the Series or DataFrame. It analyzes both numeric and object series and also the DataFrame column sets of mixed data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d283252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c31fd4",
   "metadata": {},
   "source": [
    "Pandas value_counts() function returns a Series containing counts of unique values. By default, the resulting Series is in descending order without any NA values. For example, let’s get counts for the column “Embarked” from the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age, Fare, Embarked have null values so we are doing mean, median and alphabet imputation\n",
    "df['Age'] = df.Age.fillna(df.Age.mean())\n",
    "df['Fare'] = df.Fare.fillna(df.Fare.median())\n",
    "df['Embarked'] = df['Embarked'].fillna('S') ## as it has high value count\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To check all the unique values in a dataset\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## As age cannot be in float\n",
    "df = df.astype({\"Age\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c698c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check particular passenger had a cabin or not?\n",
    "## here is null gives true wherever it finds nan value but we want true where it doesnot have nan values so we used tilt symbol\n",
    "df['Cabin_Exist'] = ~df.Cabin.isnull()\n",
    "## show data\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This lamda re.seach will give the title of the names as we choose group1 out of it\n",
    "df['Title'] = df.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n",
    "sns.countplot(x='Title', data=df);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we did this in order to know reduce the categories\n",
    "df['Title'] = df['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\n",
    "df['Title'] = df['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n",
    "                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\n",
    "sns.countplot(x='Title', data=df);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053be9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping numerical columns\n",
    "## q stands for quartile range for eg it will try to group data in 4 bins having 0-15%, 15-35%, 35-51%, 51-78% and 78-100%.\n",
    "## It segregates data based on datapoints like its checks whether the datapoint lies in first 15% of the data, or next(15-35)15% of data\n",
    "df['Age_Group'] = pd.qcut(df.Age, q=4, labels=False)\n",
    "df['Fare_range']= pd.qcut(df.Fare, q=4, labels=False)\n",
    "df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903915f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will do feature selection\n",
    "## we will remove coloumns that are unrelevant for us\n",
    "df.drop(['Cabin', 'Name', 'PassengerId', 'Ticket', 'Fare','Age'], axis=1, inplace=True)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a feature or column as we can observe Parch meaning parent and sibling meaqns child so we can create a column named Family merging 2 columns in 1\n",
    "df['Family'] = df.Parch + df.SibSp\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fa1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## again we will drop the column\n",
    "df =  df.drop(['SibSp','Parch'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58222b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we will convert categorical data into binary as it helps to get good relationship when training ML model\n",
    "df_one_hot_encoding = pd.get_dummies(df,drop_first=True)\n",
    "df_one_hot_encoding.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot_encoding = df_one_hot_encoding.drop(['Survived'], axis=1)\n",
    "df_one_hot_encoding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4432e64",
   "metadata": {},
   "source": [
    "## OUR DATA IS ALL SET TO GET TRAINED !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90d1b1",
   "metadata": {},
   "source": [
    "# Standarization Vs Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e25fd",
   "metadata": {},
   "source": [
    "##  Normalization(min-max Normalization)"
   ]
  },
  {
   "attachments": {
    "Normalization.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAABvCAMAAABFLUC0AAAAxlBMVEX///8AAAD29fbp6OlWVVf7+/vFxMXl5eX5+fnr6+vJyMmamZpLSUthYGLa2dr9/P3Qz9Cop6i7uruzsrOura7g3+BpaGnw8PAsKyyFhIWTkpMbGhxCQUIiISN8e3zBwMFycnONjI1oZ2laWVowLzAMCw0lJSY/Pj81NDWYmJkVFBZ2dnZ4eX+AfX64uLZtbnWln5w+Q02zr6kfJSptaWKfoacdIS/W0sgICxVVWV8qKS8+PEI+RldORkbq6OCYlI8kLDwAAA6B60FDAAAIJ0lEQVR4nO2cC5uiOBaGEyDIzXAHBRUQb2VVTfduT03PZWdn5///qU2C4AVLyxq17PK8TzeUEQPycXJOThIRAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAuAyxvfVSkj7oOoB3Q/q5tlVgzJ0PuhTgLcgZaZUlaWenxFyG17kc4B3YXezvljk42C2SrO51rgd4Dw+L3ZaSKIO2cXoYGtQbxt4toNhsH6Wnj9e4GOBMmHuNLh/qV7+S+0MfCU8WjrzXj5F909cM3zWQpPou14oGJkVIdU099AI/5gdFM7Gra3JGBt/5OLvs9QMM+5FbEO2VByxGdlMrUHUPB66ju5jJR595FKNG2A3CTjlmzjCeKVULK3V5SGosH4WmBk6u8jXunLhn2eGkPNwvz6fMoOz5kOmnT7nv00Uo6uBHjUcvIS+ovR+d5shZjqooh2JwitfAw/1J3mhI9A2avns+ZAfYA25scW/ECmQhooFdsWW2HOKiPtjExaxYRarhNLraF7lrcjxadw6otWbQdPzyHuEi5oiLyG2rFtFDbRE1Zd2GgojXIRxs+i1b2qDRdiUiF+S4iM5k3e0PMYh4BcJJ4h7tkuc9+4iIOh6tjjWGbr9JwIFPvAZO2keSNTgy3iBERKI5lYSI+q6ITXSaLU2kT6KVERv7UgDAeVGnfcKjzP6hg4iupKFGwp4lE82ZdnVbY10HCUkBNmMUu9iXmn6iL1TzRcTDCLB68e9w78R5IuwnGccHjqLPUTQynBHb0uwxip5lL49y0zbzcV7ECdu6BLmY9+7DsVBPG42qyDafQcbm0th1J6Kdu948itirfxrfEoIIf8m3pN6yEIaHR4TUHxE7Pc0vd/HA+YkmWqvME+YJ/DA4tR9cI82hg/GDYQ53xvHtYiF/zKXcBI4rvr1j7k54uGVIEW03qNmAftCl3AQURyxqNNLHQ0HjLtpmskVqjdpeHtLZFjG+88iU9b1YVFCcpIRpbeY979oIbgOtXLjD5DRrijubNJ2GrWzorXH2G3dThMN2sPc+HOV2KT/3lEYjTfkXtG1EVo9rXPlHIjrmzEY1vn/Do6zJN8zBJMOPjrd0LdbHMrpPXvJcsHhB7hdJv4OcKP8p/6J/VV6CJHdpUuTr+NXtbvDJn/EfAX8ZoAwHSOvjF0keZki3AoLcf8Xk3zhR52FcfstsFT/J5Oen5kOhuskndze3jyvmV+fTEH39xsL0+Qv6+gsTRVs8oO+/Ut6ejn5D6L98hOCv8VnPnAXbwZR3YBLcSRjudsu5e6JPh1rNi6BzE5m/S4j88YJ+/g/vgv05Rt+/iTTA4xcm4q/OuUX0ljsjR86wNTf/OHvkMdKdp4HOzhS33Sp15M32tYjP/+OWOH9C34cXFLHTHsINTp+I/9Bt5ZnkSbFbdEe5cXNWiUhT9pWNv51axJyL+DcX8ZxLVsaLliclJ6+JiRftgeC81049da07cduOMkzCpGepKFNMU/EQjdInimx3vgj00fIL9f9cvJwtTKftdU18kP5UU3T83TGpcN8UYu9eBv01SZNs9p/dFS0M2ZawF0w0jcHLiaadMetR4D39En06aheeyN51GvIw/8cV3w8k7kgoDnmDFod15oBS0cARm+cTkNhqg0X1pk6FKUnVIdbwUBbeluSYPWC8jZdCfXU6/szFzDHKYWWT0URaVSxVFYsD69UbwBtwLRxkrlu+aL77oojFoVniu+Kvh7GiZMixlLGBZJxXH8hE6xdbPWGYyT77bMjKad8x3aivsVN0Hwhv6qcG6+BOetQNEoWbYFzbnCEmp8bjmcjOH58pCaxxcKly5/bMbpo51VfOT+U+iYSDpc7C3kS3WWkd9eeYItmaV0FmcHhhk76wfMK8Xu7xKrksPq/YNnGffT6yxPSbOujts5A0LifVU5Htc8HAK+hiLj3Fz4gbg8FVfeapg5y/KQ9HyBPqqc0Sbn1YygNrNRroHc7E28qCtYra0LLR6jFwRMTii9rcVObnrNWKF1ZYWqseCMxJPQVZNI90NdOX2xWNY2pMSvFuhvsP4g8fNx3yDM+6tcPymnsdmhvUbaytWGyrpQWqVwvXIvI20512eA3+uuJUqadsOLDs7QRkXN1hLp8h7nAn7wbOohKROb3KN22IGPfSZlT+FRHrDj1RuA2+VcR4Pm08LIh4Ci0RWXOqIXtQilEtqRANIreSpjlVyuVD/Wn/cHP6BhHVdXPaVWbNZHNoTk+hbk4bEZUld2PzEnF5TBpPxMzCJrCR2et16HhkJn4lorQlIk+orURcdjYCG90aa0GTbvPaP6MCvIq+I6I96En8dYkKpLkmv+3c7uouBl1EEpLmdVZsbwpgDbGO+sS6ixEOyhhJynzlbcXCceBteBFeFLFfYiuJTQUrLp+CFaqumSbeg4XHvCHFY5fJseDjD3QpZh5mtceyeodyQsZoOn2mWY4nRcfr4nkRB+wUATEHuOvKxRyPmaONRBW0F/Fds3pnvCehCrwCVQ1DlfjW0AzVUPmSUD9QJWSoxDEM/rIqrbr14Wo82aja0yNptw6vXA/5Nq5O5LDKqM1OpjoS31K0WmzTUSvRHEdkeyHtdhHCfZHG6QnwNp3pnqVzHvw6ykXI29lMzSrPUPGo/ZtEdvfYylbgXXTaPbfzRB/6omWK/hQyp5chS3enZ+xOq3gnrXro8JNPz/hAHH97gkx2roFbx98euzbAIQIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3y/8BeiqGpLYxzA0AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "973c384b",
   "metadata": {},
   "source": [
    "### Here, we try to  scale down values of feature between 0 to 1\n",
    "![Normalization.png](attachment:Normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a84ef",
   "metadata": {},
   "source": [
    "### Importing MinMaxScalar library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd652fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f872a4b",
   "metadata": {},
   "source": [
    "#### This features is noted down based on various units and magnitude.So there is a huge difference between these features..its scaled down to 0 to 1. So all features are on same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2dece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot_encoding.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = list(df_one_hot_encoding.columns)\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = scaling.fit_transform(df_one_hot_encoding)\n",
    "scaled = pd.DataFrame(scaled, columns=new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37283d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaled Dataset Using MinMaxScaler\")\n",
    "scaled.head()"
   ]
  },
  {
   "attachments": {
    "z.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVUAAACUCAMAAAAUEUq5AAAAclBMVEX///8AAACAgIBiYmLr6+vd3d319fX8/PyqqqpnZ2f5+fkXFxeIiIiQkJAnJyfy8vLT09Pl5eUdHR0NDQ2Li4tLS0vExMS+vr4RERFKSkpERESxsbFZWVnNzc1sbGwuLi43NzeZmZmioqJUVFR3d3c8PDzFvsB1AAAFCUlEQVR4nO3caXeqOhQGYDajoMwICsrs//+LN2Foe7Sce5ZJq+j7fKhpl2ZlpXGzM4CiAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASGTpYRg6U0G3xCrzv9RgmYItW7GwsKsob0PFSUvPszOhrvB3VWlMZb1ojL+++YUZJV1KO6aTsYmr8hR7hchozYiqYiqrRCcZLVwhQ9uqZ9N3ibromIZNRWV4f21Oz+oJxrJfE5VyGrk2uhq7Pnv1icjLlGJLtBf42g496Y/l1CPaSGnk6hRRr/NX3qsny9+xl15grBrs88lUTg5EAlWtmN7H2VBIieJEMRs+YgXiasE+P49PjSgSb+EaGfZ0xU5Yd6QsH9i0qX/1HstZcFudZRMdp7Cq50Taz7b+WTlGOHZOR1TzlOq2rwJVs7+lNTfVmRGRPdVQVETtjzb+6TnR4sBKL7Rgd/PekMWR+a/vG1Y/pDHR7dAbOPqS61ChKO2XsMqCweGbIPFO2LX/EIhXcyLKp7zM2L/vHGDGusATXABgrCE9G8s8rC6M/tenFxuWr+rVPLD8pE7vrizk2e5UTlhMYcNWu7zhUkDA0h82Ddhs5+Q9yPf3RwKW7R6mOYBVEm19Nrm43EbfV+ewfqB9qLAuoGE24LTU3H+J6T57tWBT15PDJhe2lIauit7zGaZv8OTpzH53Mk9gqFrsSx+PEeC8HzOvU1z8z4de0LBUFeg2HVkf+KZeeFV2f20BT2LzM6sn7TyPL7ME1IlfA9cnrals9lQGLCfqG41ykcVV9i+imOqkKaN809Z0cL3o/kvfipmFVlddoiuGuq+8vSrUCew/EzVl7nndjoWTwr543ZuuBCphmgXDntU5Gwt3c9iXfq/oaZYZzlyzlCa+tTOb+KuPbsTLcef0DCTShrwfpHJY0nt8dCNeztn7bsUVxDTbcX4GMrHJb/yOE6mfZXTH9aanqdqPjQ9KpDGS8H0LIpeVwvJdD4ZIZxzH3c3e113aXR3es5Y9prUrYfZ0qOoqYun2hew/ty4sY7PszXeQ/y6rqiQ10qbvYsqvlpeGI1L/vnMPHwptWv5Mo/p6jd0K3G81jPuGO3L/zpmm2ee8Et64WD488RNWcNwircU7VQl69fckz5/JZkKbS7PNX8KwfM++J2gV24uMzaD0pP2e8skXCJw2Pkppomn8ovC5bxFyGtIWYlS4nK62+u+2cmVMl+a7mQz3zyHru/F2ycdRffgG79RwLh7+PMhsnXfLkK8uc9xYneeexeH6mmWZy369qSvSbinK7SZQnHO5JRVrJjJkHj9ww8T8p8i9ZvAh2FPjK4U3ptX5889W1sBPxpOlQc36dGtjbU8K3d2NPanvbLFDZgAAAAAAAPBQelow8w7Do1vzGtIyr7xPW/fRDXoBZhldbUfjNiBh4c3jV464YUWUzhcge8Nx+B2vlDmmuYJDPs/O7Ijicf3ROPH7sx/cntewiz/u+7Naog6L5hIEly+Pp2w/HwQEIlQaHk0z4E8VQa9KELJQepyvTmFPdEIEEJfV430zg2DPMlUc3BLXRl9upswOREIPaYYRv0V1/s7zB+DgMIIMbKxG83eepwMNDm5JwOLq3Kt8qNrIAGTgOcAYAazCIzlnvYGHgJ5nVk5b0THFpUoOk+WoWls0WkRagE6VxW9iIpYJXDbIVCWy9MxNNoaPgQoAAAAAAAAAAAAAAAAAAAAAAAAAAACwEv8BzQVKYRXQDB0AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "522f6a56",
   "metadata": {},
   "source": [
    "# Standarization(Standard Scalar)\n",
    "\n",
    "### All features are transformed in such a way that it will have properties of standard normal distribution where mean = 0 and standard deviation =1\n",
    "![z.png](attachment:z.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970018eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "st_sc = scale.fit_transform(df_one_hot_encoding)\n",
    "st_sc = pd.DataFrame(st_sc, columns=new_list)\n",
    "print(\"Scaled Dataset Using StandardScaler\")\n",
    "st_sc.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc11e7e",
   "metadata": {},
   "source": [
    "## When to use Standarization or Normalization ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94875cb8",
   "metadata": {},
   "source": [
    "#### Scenario : Machine Learning algorithms that involves Euclidean Distance and Some Depp learning techiniques where gradient descent is involved where you need to find best minimal  point. To get to that point we have algorithms like K-means, KNN, Linear Regression, Logistic Regression all deep learning networks\n",
    "#### Standarization mostly performs well than Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec358c",
   "metadata": {},
   "source": [
    "#### You don't need to perform Scaling when you go for decision Tree, Random Forest, XGBoost or all boosting technics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c6e68",
   "metadata": {},
   "source": [
    "### Now, we will convert the pandas dataframe into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a979f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Feature-Engineering with Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630acc42",
   "metadata": {},
   "source": [
    "### Create PySpark DataFrame from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef855ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(st_sc) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = sparkDF.columns\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"list_of_features\")\n",
    "data = assembler.transform(sparkDF)\n",
    "data.select(\"list_of_features\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
