{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7fe5e5",
   "metadata": {},
   "source": [
    "###  1.Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba4408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "from pyspark import SparkContext,SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bf608",
   "metadata": {},
   "source": [
    "### 2. Create SparkContext() which atcs like an entry point to the spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "730de338",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\",\"PySpark Word Count Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0911c1",
   "metadata": {},
   "source": [
    "### 3. Actual Implementation of pyspark\n",
    " - Reading a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b1e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.textFile(\"wordcountprograme.txt\").flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcddb05",
   "metadata": {},
   "source": [
    "### giving us a list of all words spliting it by spaces from the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7bd35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'All',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'pyspark',\n",
       " 'word',\n",
       " 'count',\n",
       " 'example',\n",
       " 'pyspark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'open',\n",
       " 'source',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'framework',\n",
       " 'was',\n",
       " 'example',\n",
       " 'welcome',\n",
       " 'once',\n",
       " 'again']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0669dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 1),\n",
       " ('All', 1),\n",
       " ('welcome', 1),\n",
       " ('to', 1),\n",
       " ('pyspark', 1),\n",
       " ('word', 1),\n",
       " ('count', 1),\n",
       " ('example', 1),\n",
       " ('pyspark', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('framework', 1),\n",
       " ('was', 1),\n",
       " ('example', 1),\n",
       " ('welcome', 1),\n",
       " ('once', 1),\n",
       " ('again', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = words.map(lambda word: (word, 1))\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650bf93f",
   "metadata": {},
   "source": [
    "### reduce by key internal working. It will try to check all values with key1 first then it will move to next key\n",
    "hello [1] => hello 1 </br>\n",
    "pyspark[a=1,b=1] => pyspark 2(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baefa570",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reduceByKey(lambda a,b:a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88a0a4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 1),\n",
       " ('All', 1),\n",
       " ('welcome', 2),\n",
       " ('to', 1),\n",
       " ('pyspark', 2),\n",
       " ('word', 1),\n",
       " ('count', 1),\n",
       " ('example', 2),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('framework', 1),\n",
       " ('was', 1),\n",
       " ('once', 1),\n",
       " ('again', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f784039",
   "metadata": {},
   "source": [
    "### Wide transformation leads to seperate number of stages in sparkUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e871dd7",
   "metadata": {},
   "source": [
    "### Last Step: To stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b41eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
