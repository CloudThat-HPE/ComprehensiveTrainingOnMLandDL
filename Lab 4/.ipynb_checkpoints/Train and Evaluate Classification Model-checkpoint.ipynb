{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffaad43",
   "metadata": {},
   "source": [
    "# Classification using Logistic Regression\n",
    "\n",
    "We are using the logistic regression algorithm to predict a person is diabetic or not based on the health data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7dbe8",
   "metadata": {},
   "source": [
    "To add PySpark to sys.path for running the code on the Jupyter IDE we are Using the package findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d365582",
   "metadata": {},
   "source": [
    "To perform any task on spark you need start a spark session, here we are starting a session for our logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Logistic App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625be6e",
   "metadata": {},
   "source": [
    "To start, we are loading the diabetes dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = spark.read.csv('diabetes.csv',header= True)\n",
    "diabetes.printSchema()\n",
    "diabetes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2ff7f",
   "metadata": {},
   "source": [
    "# Dropping unwanted columns\n",
    "\n",
    "We need to drop unwanted columns from the dataset. By looking into the dataset we can see columns 'PatientID' have no relevance in predicting the diabetes. To have this insight in a complex problem. we have to formulate the hypothesis and evaluation of the hypothesis should be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "colm = 'PatientID'\n",
    "db_df = diabetes.select([column for column in diabetes.columns if column not in colm])\n",
    "db_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b6624",
   "metadata": {},
   "source": [
    "# Changing the column datatype\n",
    "\n",
    "We need to change column datatype to float from the initial string datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464136ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "db_df = db_df.select(*(col(c).cast('float').alias(c) for c in db_df.columns))\n",
    "db_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb2e20",
   "metadata": {},
   "source": [
    "# Taking the count of the null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "db_df.select([count(when(col(c).isNull(), c)).alias(c) for c in db_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae455ad3",
   "metadata": {},
   "source": [
    "# List of columns to vector form\n",
    "\n",
    "We are using VectorAssembler to convert the list columns in our dataset to vector form in which all the features are grouped to vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = db_df.drop('Diabetic')\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#let's assemble our features together using vectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features.columns,\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(db_df).select('features','Diabetic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b642e",
   "metadata": {},
   "source": [
    "# Splitting the data into training and testing datasets\n",
    "\n",
    "The dataset in vector form is now splitting into train and test datset fractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f51c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = output.randomSplit([0.75, 0.25])\n",
    "train.show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca07330",
   "metadata": {},
   "source": [
    "# Train the Logistic Regression Model\n",
    "\n",
    "We are using Logistic Regression model for classsification problem. In the following step we training the logistic regression model with labels and features. This kind of training where both labels and features are used are known as supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'Diabetic', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c557c228",
   "metadata": {},
   "source": [
    "# Coefficients of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "coeff = np.sort(lrModel.coefficients)\n",
    "plt.plot(coeff)\n",
    "plt.ylabel('Coefficients')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3b411",
   "metadata": {},
   "source": [
    "# Predictions and labels\n",
    "\n",
    "To compare the predictions with actual labels we use model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.show(10)\n",
    "predictions = predictions.withColumnRenamed(\"Diabetic\",\"label\")\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9c837",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "y_true = predictions.select(['label']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import   confusion_matrix\n",
    "from sklearn import metrics\n",
    "#print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion Matrix\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32fafc",
   "metadata": {},
   "source": [
    "# Accuracy, Precision, Recall, F1-Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "#print('Area Under ROC', evaluator.evaluate(predictions))\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "acc = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "f1 = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"f1\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "#auc = evaluator.evaluate(predictionAndTarget)\n",
    "\n",
    "print('Precision', weightedPrecision)\n",
    "print('Accuracy', acc)\n",
    "print('F1-Score', f1)\n",
    "print('Recall', weightedRecall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cf8f3",
   "metadata": {},
   "source": [
    "# Area Under ROC\n",
    "ROC Curve is plotting using model.summary and AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ee8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Area UnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c56e1",
   "metadata": {},
   "source": [
    "# Train the Decision Tree Classification Model\n",
    "\n",
    "We are using Decision Tree model for classsification problem. In the following step we training the decision tree model with labels and features. This kind of training where both labels and features are used are known as supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f3086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Diabetic\", featuresCol=\"features\", maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions_dt = dtModel.transform(test)\n",
    "predictions_dt.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72bd785",
   "metadata": {},
   "source": [
    "# Accuracy, Precision, Recall, and F1-Score of Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef145a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"Diabetic\", predictionCol=\"prediction\")\n",
    "#print('Area Under ROC', evaluator.evaluate(predictions))\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "acc = evaluatorMulti.evaluate(predictions_dt, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "f1 = evaluatorMulti.evaluate(predictions_dt, {evaluatorMulti.metricName: \"f1\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(predictions_dt, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(predictions_dt, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "#auc = evaluator.evaluate(predictionAndTarget)\n",
    "\n",
    "print('Precision', weightedPrecision)\n",
    "print('Accuracy', acc)\n",
    "print('F1-Score', f1)\n",
    "print('Recall', weightedRecall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868075e2",
   "metadata": {},
   "source": [
    "# Confusion Matrix of Decision Tree Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93937a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "y_true = predictions_dt.select(['Diabetic']).collect()\n",
    "y_pred = predictions_dt.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import   confusion_matrix\n",
    "from sklearn import metrics\n",
    "#print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion Matrix\", confusion_matrix(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08c082",
   "metadata": {},
   "source": [
    "# Area Under ROC for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f015169",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = predictions_dt.withColumnRenamed(\"Diabetic\",\"label\")\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions_dt, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d1791",
   "metadata": {},
   "source": [
    "# Binary Classification Using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"Diabetic\", featuresCol=\"features\", maxDepth = 3)\n",
    "rfModel = rf.fit(train)\n",
    "predictions_rf = rfModel.transform(test)\n",
    "predictions_rf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d73d2b",
   "metadata": {},
   "source": [
    "# Precision, Accuracy, Recall, and F1-Score of Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7129d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"Diabetic\", predictionCol=\"prediction\")\n",
    "#print('Area Under ROC', evaluator.evaluate(predictions))\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "acc = evaluatorMulti.evaluate(predictions_rf, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "f1 = evaluatorMulti.evaluate(predictions_rf, {evaluatorMulti.metricName: \"f1\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(predictions_rf, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(predictions_rf, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "#auc = evaluator.evaluate(predictionAndTarget)\n",
    "\n",
    "print('Precision', weightedPrecision)\n",
    "print('Accuracy', acc)\n",
    "print('F1-Score', f1)\n",
    "print('Recall', weightedRecall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d360d",
   "metadata": {},
   "source": [
    "# Confusion Matrix for Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "y_true = predictions_rf.select(['Diabetic']).collect()\n",
    "y_pred = predictions_rf.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import   confusion_matrix\n",
    "from sklearn import metrics\n",
    "#print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion Matrix\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1f538",
   "metadata": {},
   "source": [
    "# Area Under ROC for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245eb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = predictions_rf.withColumnRenamed(\"Diabetic\",\"label\")\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a7678d",
   "metadata": {},
   "source": [
    "# Multi Class Classification using Random Forest Algorithm\n",
    "\n",
    "We are using the decision tree algorithm to predict the species of flower. The iris dataset used here includes three iris species with 50 samples each as well as some properties about each flower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3b955",
   "metadata": {},
   "source": [
    "To perform any task on spark you need start a spark session, here we are starting a session for our multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Multi App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab964892",
   "metadata": {},
   "source": [
    "To start, we are loading the Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e11217",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ismayilsiyad/hpe_ml/main/IRIS.csv\n",
    "iris = spark.read.csv('IRIS.csv', header = True, inferSchema = True)\n",
    "iris.printSchema()\n",
    "iris.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02995161",
   "metadata": {},
   "source": [
    "# Changing the column datatype\n",
    "\n",
    "We need to change column datatype to float from the initial string datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "features = iris.drop('species')\n",
    "features = features.select(*(col(c).cast('float').alias(c) for c in features.columns))\n",
    "features.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d442e45",
   "metadata": {},
   "source": [
    "# Taking the count of the null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "features.select([count(when(col(c).isNull(), c)).alias(c) for c in features.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88aab9",
   "metadata": {},
   "source": [
    "# List of columns to vector form\n",
    "\n",
    "We are using VectorAssembler to convert the list columns in our dataset to vector form in which all the features are grouped to vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86920ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "#let's assemble our features together using vectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features.columns,\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(iris).select('features','species')\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = 'species', outputCol = 'labelIndex')\n",
    "df = label_stringIdx.fit(output).transform(output)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ecee4c",
   "metadata": {},
   "source": [
    "# Splitting the data into training and testing datasets\n",
    "\n",
    "The dataset in vector form is now splitting into train and test datset fractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b62e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = df.randomSplit([0.7, 0.3])\n",
    "train.show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b424e6e",
   "metadata": {},
   "source": [
    "# Train the Classification Model\n",
    "\n",
    "We are using Random Forest model for multiclass classification problem. In the following step we training the random forest model with labels and features. This kind of training where both labels and features are used are known as supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'labelIndex')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select('features','labelIndex', 'rawPrediction', 'prediction', 'probability').show(500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c094e",
   "metadata": {},
   "source": [
    "# Training Accuracy and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f48c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8322e",
   "metadata": {},
   "source": [
    "# Predictions and labels\n",
    "\n",
    "To compare the predictions with actual labels we use model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfModel.transform(test)\n",
    "predictions.show(10)\n",
    "predictions = predictions.withColumnRenamed(\"labelIndex\",\"label\")\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a94272",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e680c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "preds_and_labels = predictions.select(['prediction','label']).withColumn('label', F.col('label').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1fdaa2",
   "metadata": {},
   "source": [
    "# Accuracy, Precision, Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "#print('Area Under ROC', evaluator.evaluate(predictions))\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "acc = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "f1 = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"f1\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "#auc = evaluator.evaluate(predictionAndTarget)\n",
    "\n",
    "print('Precision', weightedPrecision)\n",
    "print('Accuracy', acc)\n",
    "print('F1-Score', f1)\n",
    "print('Recall', weightedRecall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
