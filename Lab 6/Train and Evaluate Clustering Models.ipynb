{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676bda76",
   "metadata": {},
   "source": [
    "# Lab -6 Clustering Example - Customer Grouping\n",
    "\n",
    "We are grouping a dataset with each unique customer has already been given the recency, frequency, and monetary values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd7bd4",
   "metadata": {},
   "source": [
    "To add PySpark to sys.path for running the code on the Jupyter IDE we are Using the package findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd47b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "findspark.find()\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b7dd4",
   "metadata": {},
   "source": [
    "To perform any task on spark you need start a spark session, here we are starting a session for our logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512efc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Clustering App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bcf83",
   "metadata": {},
   "source": [
    "To start, we are loading the customer dataset\n",
    "\n",
    "\n",
    "The features are\n",
    "\n",
    "Recency: How recently customers made their purchase.\n",
    "\n",
    "Frequency: For simplicity, weâ€™ll count the number of times each customer made a purchase.\n",
    "\n",
    "Monetary: The total amount of money they spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust = spark.read.csv('retail_loyalty_rfm.csv',header= True)\n",
    "cust.printSchema()\n",
    "cust.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8496cb2",
   "metadata": {},
   "source": [
    "# Changing the column datatype\n",
    "\n",
    "We need to change column datatype to float from the initial string datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42495a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "cust1 = cust.select(*(col(c).cast('float').alias(c) for c in cust.columns))\n",
    "cust1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc767f5",
   "metadata": {},
   "source": [
    "# Dropping unwanted columns\n",
    "\n",
    "We need to drop unwanted columns from the dataset. To have this insight in a complex problem. we have to formulate the hypothesis and evaluation of the hypothesis should be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89519e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colm = ['R_Score','F_Score', 'M_Score','RFM_Score','RFM_ScoreGroup','Loyalty']\n",
    "cust2 = cust1.select([column for column in cust1.columns if column not in colm])\n",
    "cust2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3214f7",
   "metadata": {},
   "source": [
    "# Taking the count of the null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02219953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "cust2.select([count(when(col(c).isNull(), c)).alias(c) for c in cust2.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ca1c2",
   "metadata": {},
   "source": [
    "# Visualizing the data\n",
    "\n",
    "To have an insight about the dataset we use the python library seaborn. The plots indicate the histogram of each features of dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cust_df = cust2.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 8))\n",
    "\n",
    "# Recency distribution plot\n",
    "sns.histplot(cust_df['Recency'], kde=True, ax=ax[0])\n",
    "\n",
    "# Frequency distribution plot\n",
    "sns.histplot(cust_df.query('Frequency < 1000')['Frequency'], kde=True, ax=ax[1])\n",
    "\n",
    "# Monetary distribution plot\n",
    "sns.histplot(cust_df.query('Monetary < 10000')['Monetary'], kde=True, ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57522708",
   "metadata": {},
   "source": [
    "The histogram figures show how the recency, frequency and monetary values are distributed in the given dataset. Most of the datapoints are distributed in the initial values of the graph. If any value is not significant we can remove it as an outlier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3fb49",
   "metadata": {},
   "source": [
    "# Remove zero and negative numbers\n",
    "\n",
    "For smooth working of algorithm, lets get rid of negative and zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca11b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "rfm_data = (\n",
    "    cust2.withColumn(\"Monetary\", \n",
    "        F.when(F.col(\"Monetary\") <= 0, 1)\n",
    "         .otherwise(F.col(\"Monetary\")))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c66810",
   "metadata": {},
   "source": [
    "# Vectorization and Scaler\n",
    "\n",
    "Because of their skewed behavior and different scales and ranges, the three features (recency, frequency, and monetary) must be standardized, so that machine learning algorithms can identify the trends between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying feature columns\n",
    "features = rfm_data.columns[1:]\n",
    "# vectorize all the features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"rfm_features\")\n",
    "assembled_data = assembler.transform(rfm_data)\n",
    "assembled_data = assembled_data.select(\n",
    "    'CustomerID', 'rfm_features')\n",
    "# Standardization\n",
    "scaler = StandardScaler(\n",
    "inputCol='rfm_features',\n",
    "outputCol='features')\n",
    "data_scale = scaler.fit(assembled_data)\n",
    "scaled_data = data_scale.transform(assembled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157afd93",
   "metadata": {},
   "source": [
    "### Applying K-Means Clustering\n",
    "\n",
    "Next, we're going to run k-means on the entire dataset one final time with the cluster number 'k' equal to 3 and create a column named 'prediction' for each customer's predicted cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ce5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "k_means = KMeans(featuresCol='features', k=3)\n",
    "model = k_means.fit(scaled_data)\n",
    "predictions = model.transform(scaled_data)\n",
    "\n",
    "result = predictions.select('CustomerID', 'prediction')\n",
    "result.show()\n",
    "\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a26ce5",
   "metadata": {},
   "source": [
    "In order to inspect the results with charts, let's join the prediction with the starting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d711e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_score = spark.read.csv('retail_loyalty_rfm.csv', inferSchema=True, header=True)\n",
    "rfm_score = rfm_score.select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"RFM_Score\", \"RFM_ScoreGroup\", \"Loyalty\")\n",
    "combined_result = result.join(rfm_score, on='CustomerID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df = combined_result.toPandas()\n",
    "\n",
    "combined_result_df.to_csv('combined_result_k4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28b423",
   "metadata": {},
   "source": [
    "# Visualizing the Results\n",
    "\n",
    "We are visualizing the clusters, and the population in each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 8))\n",
    "\n",
    "sns.countplot(x=\"prediction\", data=combined_result_df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{}'.format(p.get_height()), (p.get_x() + 0.3, p.get_height() + 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe3506",
   "metadata": {},
   "source": [
    "We plot the recency, frequency, and monetary values of each cluster in a box plot, in order to better understand the prediction result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = combined_result.toPandas()\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 12))\n",
    "sns.boxplot(x='prediction', y='Recency', data=analysis_df, ax=ax[0])\n",
    "sns.boxplot(x='prediction', y='Frequency', data=analysis_df, ax=ax[1])\n",
    "sns.boxplot(x='prediction', y='Monetary', data=analysis_df, ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824a33d",
   "metadata": {},
   "source": [
    "The boxplots of cluster values corresponds to each feature is shown. Using these boxplots we can identify the possible outliers corresponds to each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803d0b1",
   "metadata": {},
   "source": [
    "# Visualizing Clusters\n",
    "Visualizing the clusters by combining two features as a 2D plot will give a clear view about the grouping strategies of datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85434336",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_result_df = combined_result_df#.query('Monetary < 100000').query('Frequency < 3000')\n",
    "\n",
    "# Monetary vs Frequency (combined)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sns.scatterplot(x='Recency', y='Monetary', data=selected_result_df, hue='prediction', palette=\"deep\")\n",
    "\n",
    "# Monetary vs Frequency (combined)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sns.scatterplot(x='Recency', y='Frequency', data=selected_result_df, hue='prediction', palette=\"deep\")\n",
    "\n",
    "# Monetary vs Frequency (combined)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sns.scatterplot(x='Monetary', y='Frequency', data=selected_result_df, hue='prediction', palette=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8bd9bb",
   "metadata": {},
   "source": [
    "# 3D Visualization of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0810b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threedee = plt.figure(figsize=(12,10)).gca(projection='3d')\n",
    "threedee.scatter(selected_result_df.Recency, selected_result_df.Monetary, selected_result_df.Frequency, c=selected_result_df.prediction)\n",
    "threedee.set_xlabel('Recency')\n",
    "threedee.set_ylabel('Monetary')\n",
    "threedee.set_zlabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b247385",
   "metadata": {},
   "source": [
    "3D Visualization can give a good insight about how datagrouping is done corresponds to each feature in a single plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df20ef",
   "metadata": {},
   "source": [
    "# Applying Bisecting KMeans Clustering\n",
    "\n",
    "Next, we're going to run k-means on the entire dataset one final time with the cluster number 'k' equal to 4 and create a column named 'prediction' for each customer's predicted cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(3)\n",
    "model = bkm.fit(scaled_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(scaled_data)\n",
    "\n",
    "\n",
    "result = predictions.select('CustomerID', 'prediction')\n",
    "result.show()\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8ff94",
   "metadata": {},
   "source": [
    "In order to inspect the results with charts, let's join the prediction with the starting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_score = spark.read.csv('retail_loyalty_rfm.csv', inferSchema=True, header=True)\n",
    "rfm_score = rfm_score.select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"RFM_Score\", \"RFM_ScoreGroup\", \"Loyalty\")\n",
    "combined_result = predictions.join(rfm_score, on='CustomerID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6928aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result_df = combined_result.toPandas()\n",
    "\n",
    "combined_result_df.to_csv('combined_result_k4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04562b86",
   "metadata": {},
   "source": [
    "# Visualizing the Results\n",
    "\n",
    "We are visualizing the clusters, and the population in each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 8))\n",
    "\n",
    "sns.countplot(x=\"prediction\", data=combined_result_df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{}'.format(p.get_height()), (p.get_x() + 0.3, p.get_height() + 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c096ae8",
   "metadata": {},
   "source": [
    "We plot the recency, frequency, and monetary values of each cluster in a box plot, in order to better understand the prediction result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = combined_result.toPandas()\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 12))\n",
    "sns.boxplot(x='prediction', y='Recency', data=analysis_df, ax=ax[0])\n",
    "sns.boxplot(x='prediction', y='Frequency', data=analysis_df, ax=ax[1])\n",
    "sns.boxplot(x='prediction', y='Monetary', data=analysis_df, ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8676e",
   "metadata": {},
   "source": [
    "The boxplots of cluster values corresponds to each feature is shown. Using these boxplots we can identify the possible outliers corresponds to each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dfecc4",
   "metadata": {},
   "source": [
    "# Visualizing Clusters\n",
    "Visualizing the clusters by combining two features as a 2D plot will give a clear view about the grouping strategies of datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07666b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_result_df = combined_result_df#.query('Monetary < 100000').query('Frequency < 3000')\n",
    "\n",
    "# Monetary vs Frequency (combined)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sns.scatterplot(x='Recency', y='Monetary', data=selected_result_df, hue='prediction', palette=\"deep\")\n",
    "\n",
    "# Monetary vs Frequency (combined)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sns.scatterplot(x='Recency', y='Frequency', data=selected_result_df, hue='prediction', palette=\"deep\")\n",
    "\n",
    "# Monetary vs Frequency (combined)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sns.scatterplot(x='Monetary', y='Frequency', data=selected_result_df, hue='prediction', palette=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f9ef8",
   "metadata": {},
   "source": [
    "# 3D Visualization of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure(figsize=(12,10)).gca(projection='3d')\n",
    "threedee.scatter(selected_result_df.Recency, selected_result_df.Monetary, selected_result_df.Frequency, c=selected_result_df.prediction)\n",
    "threedee.set_xlabel('Recency')\n",
    "threedee.set_ylabel('Monetary')\n",
    "threedee.set_zlabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f43a3e",
   "metadata": {},
   "source": [
    "3D Visualization can give a good insight about how datagrouping is done corresponds to each feature in a single plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58250b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
