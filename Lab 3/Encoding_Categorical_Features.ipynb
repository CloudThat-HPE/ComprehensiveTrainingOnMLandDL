{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Encoding Categorical Features in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc1 = SparkSession.builder.appName(\"Lab-03_Encoding_categorical_features\").getOrCreate()\n",
    "sc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc1.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StringIndexer <br>\n",
    "StringIndexer encodes a string column of labels to a column of label indices. <br>\n",
    "Four ordering options are supported: <br>\n",
    "1. “frequencyDesc”: descending order by label frequency (most frequent label assigned 0), \n",
    "2. “frequencyAsc”: ascending order by label frequency (least frequent label assigned 0), \n",
    "3. “alphabetDesc”: descending alphabetical order, and \n",
    "4. “alphabetAsc”: ascending alphabetical order (default = “frequencyDesc”). <br>\n",
    "\n",
    "Note that in case of equal frequency when under “frequencyDesc”/”frequencyAsc”, the strings are further sorted by alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\", stringOrderType='alphabetDesc')\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoder <br>\n",
    "\n",
    "One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values.\n",
    "\n",
    "For string type input data, it is common to encode categorical features using StringIndexer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc1.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output comprises of 3 values. <br>\n",
    "1. First value indicates the length of the vector.\n",
    "2. Second value indicates an array of indices or positions where non zero entries are found.\n",
    "3. Third value indicates an array that tells which numbers are found in the indices indicated by the array in 2.\n",
    "\n",
    "<br>\n",
    "Example: (2, [1], [1.0]) denotes the vector is of length '2' (two), has a value of 1 present at the index 1 or location 1. Therefore, the one hot vector is '01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1 = OneHotEncoder(inputCol='categoryIndex', outputCol='One-hot-vector')\n",
    "model1 = encoder1.fit(indexed)\n",
    "encoded1 = model1.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot-encoding is a quintessential step for preparing any dataset for machine learning modeling. This is one of the most common steps in any feature pre-processing pipeline. One-hot encoding turns categorical data into a binary vector representation. This approach creates a new column for each unique value in the original category column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"One_Hot_Encoding\").getOrCreate() \n",
    "df = spark.read.option(\"header\", True).csv(\"sample.csv\") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common PySpark implementation of One-Hot-Encoding\n",
    "PySpark has a quite simple implementation for one-hot-encoding. It goes as follows:\n",
    "\n",
    "- Convert the String Values to Numeric Labels/Indices\n",
    "- One-Hot-Encode the Numeric Labels to a VectorUDT (pyspark.ml.linalg.VectorUDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ##  import the required libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#   ##  numeric indexing for the strings (indexing starts from 0)\n",
    "indexer = StringIndexer(inputCol=\"Color\", outputCol=\"ColorNumericIndex\")\n",
    "\n",
    "#   ##  fit the indexer model and use it to transform the strings into numeric indices\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "#   ##  one-hot-encoding the numeric indices\n",
    "ohe = OneHotEncoder(inputCol=\"ColorNumericIndex\", outputCol=\"ColorOHEVector\")\n",
    "\n",
    "#   ##  fit the ohe model and use it to transform the numeric indices into ohe vectors\n",
    "df = ohe.fit(df).transform(df)\n",
    "\n",
    "df.show()\n",
    "#   ##  get datatype of the ohe vector column\n",
    "print(df.schema[\"ColorOHEVector\"].dataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretable One Hot Encoding in PySpark\n",
    "To create an interpretable One Hot Encoder, we need to create a separate column for each distinct value. This is easily done using pyspark dataframe’s in-builtwithColumn function by passing a UDF (user-defined function) as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ##  import the required libraries\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is what we would need to do:\n",
    "\n",
    "Gather all the distinct values in the column that needs to be one-hot-encoded\n",
    "For each of the gathered values create a new column with column name in the format <<original column name>>_<<distinct value>> representing the presence (1) or absence (0) of the distinct value in the record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Pandas for creating dataframe. can we use spark native way for same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ##  gather the distinct values\n",
    "distinct_values = df.select(\"Color\")\\\n",
    "                    .distinct()\\\n",
    "                    .rdd\\\n",
    "                    .flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ##  for each of the gathered values create a new column \n",
    "for distinct_value in distinct_values:\n",
    "    function = udf(lambda item: \n",
    "                   1 if item == distinct_value else 0, \n",
    "                   IntegerType())\n",
    "    new_column_name = \"Color\"+'_'+distinct_value\n",
    "    df = df.withColumn(new_column_name, function(col(\"Color\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f42e622176c297619789989c2c6c0b7f411b349ee1112ea7e5531bb10e594a11"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
