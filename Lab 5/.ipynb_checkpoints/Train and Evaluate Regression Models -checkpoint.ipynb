{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec726a0e",
   "metadata": {},
   "source": [
    "# Lab-5 Linear Regression model\n",
    "\n",
    "We are using linear regression on spark to predict the price of real estate property based on the features like age, stores, latitude, longitude, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8624d7",
   "metadata": {},
   "source": [
    "To add PySpark to sys.path for running the code on the Jupyter IDE we are Using the package findspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11aa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "findspark.find()\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50990d4",
   "metadata": {},
   "source": [
    "To perform any task on spark you need start a spark session, here we are starting a session for our linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Linear App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20a1b1",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Exploration\n",
    "\n",
    "Now we started our spark session. To start building our linear regression model we need to load and process the real estate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8573d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate = spark.read.csv('Real estate.csv',header= True)\n",
    "real_estate.printSchema()\n",
    "real_estate.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b40a85",
   "metadata": {},
   "source": [
    "# Dropping unwanted columns\n",
    "\n",
    "We need to drop unwanted columns from the dataset. By looking into the dataset we can see columns 'No.' and  X1 transaction date' have no relevance in predicting the price. To have this insight in a complex problem. we have to formulate the hypothesis and evaluation of the hypothesis should be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "colm = ['No','X1 transaction date']\n",
    "re_df = real_estate.select([column for column in real_estate.columns if column not in colm])\n",
    "re_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e49e3",
   "metadata": {},
   "source": [
    "# Changing the column datatype\n",
    "\n",
    "We need to change column datatype to float from the initial string datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "re_df = re_df.select(*(col(c).cast('float').alias(c) for c in re_df.columns))\n",
    "re_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7124b11",
   "metadata": {},
   "source": [
    "# Taking the count of the null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660bdaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "re_df.select([count(when(col(c).isNull(), c)).alias(c) for c in re_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb42d9",
   "metadata": {},
   "source": [
    "Since the count is zero we need not do anything further. If the count is nonzero we have to remove or substitute these values. One more preproceessing that we re doing is to reduce the lengthy names of each column to managable smaller names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "oldColumns = re_df.schema.names\n",
    "newColumns = ['Age','Distance_2_MRT','Stores','Latitude','Longitude','Price']\n",
    "\n",
    "re_df = reduce(lambda re_df, idx: re_df.withColumnRenamed(oldColumns[idx], newColumns[idx]),range(len(oldColumns)), re_df)\n",
    "re_df.printSchema()\n",
    "re_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413fd8b0",
   "metadata": {},
   "source": [
    "# Visualizing the data\n",
    "\n",
    "We are using an open visualization library Matplotlib for visualizing our real estate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ae6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This ensures plots are displayed inline in the Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the label column\n",
    "\n",
    "label = re_df.select(col(\"price\"))\n",
    "label = label.select(\"price\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "# Create a figure for 2 subplots (2 rows, 1 column)\n",
    "fig, ax = plt.subplots(2, 1, figsize = (9,12))\n",
    "\n",
    "# Plot the histogram   \n",
    "ax[0].hist(label, bins=100)\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the boxplot   \n",
    "ax[1].boxplot(label, vert=False)\n",
    "ax[1].set_xlabel('Price')\n",
    "\n",
    "# Add a title to the Figure\n",
    "fig.suptitle('Real Estate Property Price Distribution')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d87005",
   "metadata": {},
   "source": [
    "# List of columns to vector form\n",
    "\n",
    "We are using VectorAssembler to convert the list columns in our dataset to vector form in which all the features are grouped to vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba707570",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = re_df.drop('Price')\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#let's assemble our features together using vectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features.columns,\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(re_df).select('features','Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987377d4",
   "metadata": {},
   "source": [
    "# Splitting the data into training and testing datasets\n",
    "\n",
    "The dataset in vector form is now splitting into train and test datset fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20268bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = output.randomSplit([0.75, 0.25])\n",
    "train.show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d47ab",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "Now we are using linear regression model from pyspark.ml library, and loading our data to the model for training. The coefficients and intercepts(biases) are printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lin_reg = LinearRegression(featuresCol = 'features', labelCol='Price')\n",
    "linear_model = lin_reg.fit(train)\n",
    "print(\"Coefficients: \" + str(linear_model.coefficients))\n",
    "print(\"\\nIntercept: \" + str(linear_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599af929",
   "metadata": {},
   "source": [
    "# Evaluation of Model\n",
    "The summary of the training process is listed below using model.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b69d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSummary = linear_model.summary\n",
    "print(\"RMSE: %f\" % trainSummary.rootMeanSquaredError)\n",
    "#print(\"\\nr2: %f\" % trainSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0feae",
   "metadata": {},
   "source": [
    "# Regression Evaluation\n",
    "\n",
    "We can compare actual price with the predicted price based on error values such as Mean Absolute Error and Root Mean Squared Error Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = linear_model.transform(test)\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator1 = RegressionEvaluator(\n",
    "    labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator1.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc24981",
   "metadata": {},
   "source": [
    "# R Squared (R2) value\n",
    "\n",
    "We can use RegressionEvaluator for the coefficient of determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adccd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"Price\",metricName=\"r2\")\n",
    "print(\"Coefficient of determination = %g\" % pred_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8525df",
   "metadata": {},
   "source": [
    "# Random Forest Regression Model\n",
    "\n",
    "Now we are using Random Forest regression model from pyspark.ml library, and loading our data to the model for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50bff84",
   "metadata": {},
   "source": [
    "\n",
    "Automatically identify categorical features, and index them. Set maxCategories so features with > 4 distinct values are treated as continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fe34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(output)\n",
    "\n",
    "output = output.withColumnRenamed(\"Price\",\"label\")\n",
    "train,test = output.randomSplit([0.75, 0.25])\n",
    "train.show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "#lin_reg = LinearRegression(featuresCol = 'features', labelCol='Price')\n",
    "#linear_model = lin_reg.fit(train)\n",
    "#print(\"Coefficients: \" + str(linear_model.coefficients))\n",
    "#print(\"\\nIntercept: \" + str(linear_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576374db",
   "metadata": {},
   "source": [
    "# Evaluation of Model\n",
    "The summary of the training process is listed below using model.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator1 = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator1.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ceed52",
   "metadata": {},
   "source": [
    "# R Squared (R2) value\n",
    "\n",
    "We can use RegressionEvaluator for the coefficient of determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ec3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator()\n",
    "evaluator.setPredictionCol(\"label\")\n",
    "acc=evaluator.evaluate(predictions)\n",
    "mae=evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"label\",metricName=\"r2\")\n",
    "print(\"Coefficient of determination = %g\" % pred_evaluator.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fcd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
