{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The following command adds the pyspark to sys.path at runtime. If the pyspark is not on the system path by default. It also prints the path of the spark. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "print(findspark.find())\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **Create a Spark Session**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "#sc = SparkContext('local')\n",
    "#spark = SparkSession(sc)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Lab-06_Hyperparameter_Tuning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Import the relevant package from pyspark\n",
    "1. ParamGridBuilder is used to define the parameter Grid in hyperparameter tuning.\n",
    "2. TrainValidationSplit and CrossValidator is used to split the data-set.\n",
    "3. RegressionEvaluator is used evaluate the regression model during hyperparameter tuning. <br>\n",
    "    3. a. For a two-class classification model use BinaryClassificationEvaluator. <br>\n",
    "    3. b. For multiclass classification use MulticlassClassificationEvaluator or MultilabelClassificationEvaluator. <br>\n",
    "    3. c. For a clustering model use ClusteringEvaluator.\n",
    "4. VectorAssembler is a transformer used to merge multiple columns into a vector column.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "toyota.csv <br>\n",
    "\n",
    "1. model: identifies the model of the vehicle <br>\n",
    "2. year: denotes the registration year of the vehicle <br>\n",
    "3. price: denoted the in euros <br>\n",
    "4. transmission: denotes the type of gearbox (i.e., manual and automatic) <br>\n",
    "5. mileage: denotes the distance covered by the vehicle in miles<br>\n",
    "6. fuel type: denotes the of fuel for the vehicle <br>\n",
    "7. tax: denotes the amount of road tax paid for the vehicle <br>\n",
    "8. mpg: denotes the number of miles per gallon of fuel covered by the vehicle <br>\n",
    "9. engine size: denotes engine capacity in number of liters <br>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"C:\\\\Users\\\\CloudThat\\\\Documents\\\\HPE_documentation\\\\Day_03\\\\toyota.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "View the first 20 rows of the dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "View the schema of the dataframe\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine a count of the unique values in each column of the dataframe. To view the unique values, remove the function count() in the below cell.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in each column are \\n\")\n",
    "for col in df.columns:\n",
    "    print(col, df.select(col).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Check for any missing values in the dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCheck for Missing values')\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Check for any missing values in the dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCheck for Null values')\n",
    "\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Encode the categorical columns in the dataframe to a numerical value using label indexing. For example, two string values such as 'A' and 'B' will be encoded as '1' and '2'. New columns will be added in the dataframe corresponding to each of the encoded columns.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexer = [StringIndexer(inputCol='model', outputCol=\"m_index\"), \\\n",
    "    StringIndexer(inputCol='transmission', outputCol=\"t_index\"), \\\n",
    "        StringIndexer(inputCol='fuelType', outputCol=\"ft_index\")]\n",
    "        \n",
    "pipeline = Pipeline(stages=indexer)\n",
    "\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Dropping the columns with the categorical values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed = df_indexed.drop('model', 'transmission', 'fuelType')\n",
    "df = df_indexed\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Casting all the cloumns from any datatype to 'float' type.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in df_indexed.columns:\n",
    "    df_indexed = df_indexed.withColumn(col_name, col(col_name).cast('float'))\n",
    "\n",
    "df_indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Determine if the duplicate rows are present or not. <br>\n",
    "2. Drop or delete the duplicated rows in the dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Data before dropping duplicates is \", df.count(), len(df.columns))\n",
    "\n",
    "print(\"Number of distinct columns are \", df.distinct().count())\n",
    "df = df.dropDuplicates()\n",
    "df_indexed = df_indexed.dropDuplicates()\n",
    "\n",
    "print(\"Size of Data after dropping duplicates is \",df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine the statistical attributes of the columns in the dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine the Pearson correlation matrix for the columns of the Dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=df_indexed.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df_indexed).select(vector_col)\n",
    "\n",
    "matrix = Correlation.corr(df_vector, vector_col).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Display the row of the Pearson correlation matrix corresponding to the row of 'price' (i.e., label column)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pearson correlation matrix:\\n\" + str(matrix[0].toArray()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Select the top three features that are highly correlated with the label column (i.e., 'price'), and drop the remaining columns.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_indexed.drop('year','mileage', 'tax', 'mpg', 'ft_index') \n",
    "df_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Apply Min-Max normalization technique to the selected features.\n",
    "2. Create a vector of features using Vector Assembler.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['engineSize','m_index','t_index'], outputCol='features')\n",
    "df_input = assembler.transform(df_input)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "df_input = scaler.fit(df_input).transform(df_input)\n",
    "df_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Apply Min-Max normalization on the 'price' column.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['price'], outputCol='label')\n",
    "df_input = assembler.transform(df_input)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"label\", outputCol=\"scaled_price\")\n",
    "df_input = scaler.fit(df_input).transform(df_input)\n",
    "df_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The label column ('price') should be of type 'float' or 'double' for providing it to the regression model. <br>\n",
    "Since the Min-Max normalization generates the output as a vector. <br>\n",
    "We convert the label column to a data type of 'float'\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "#from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "df_data = df_input.select('scaled_features','scaled_price')\n",
    "df_data.printSchema()\n",
    "\n",
    "udf1 = F.udf(lambda x : float(x[0]),FloatType())\n",
    "df_data = df_input.withColumn('scaled_price',udf1('scaled_price').alias('scaled_price_')).select('scaled_features','scaled_price')\n",
    "df_data.show(5)\n",
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We divide the Data-set into Training and Testing parts in a ratio of 70-30.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_data.randomSplit([0.7, 0.3], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(n = 5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Configure a Linear Regression Model by specifying the input column, output column, and the maximum number of iterations. <br>\n",
    "2. Define the Parameter Grid for the Linear Regression Model.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol='scaled_price', maxIter=100)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0.2, 0.8]) \\\n",
    "    .addGrid(lr.regParam, [0.3]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Use the CrossValidator for hyperparmeter tuning for the Linear Regression model using the defined parameter grid.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"scaled_price\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=2,\n",
    "                          parallelism=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Train the Linear regression model with crossvalidator\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Obtain the best model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvModel.bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Display the details of all the models trained \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(cvModel.avgMetrics, cvModel.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine the Mean squared error of the best model on the test data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_summary = cvModel.bestModel.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e_summary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine the predictions of the best model on the test data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = cvModel.bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the hyperparmeter tuning process for the Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Configure the Random Forest Regressor <br>\n",
    "2. Define Parameter Grid <br>\n",
    "3. Use the Cross Validator for hyperparameter tuning <br>\n",
    "4. Determine the best model <br>\n",
    "5. Display the details of all the models trained <br>\n",
    "6. Evaluate the best model on the test data and display the MSE <br>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol='scaled_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [1, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"scaled_price\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=2,\n",
    "                          parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvModel.bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(cvModel.avgMetrics, cvModel.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = cvModel.bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"scaled_price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "485ad2d47d78171392804dc4ab56d68e22869fca1387c3a77d012fec7abc62c2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
