{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The following command adds the pyspark to sys.path at runtime. If the pyspark is not on the system path by default. It also prints the path of the spark. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "print(findspark.find())\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **Create a Spark Session**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/21 10:09:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "#sc = SparkContext('local')\n",
    "#spark = SparkSession(sc)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Lab-07_Hyperparameter_Tuning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Import the relevant package from pyspark\n",
    "1. ParamGridBuilder is used to define the parameter Grid in hyperparameter tuning.\n",
    "2. TrainValidationSplit and CrossValidator is used to split the data-set.\n",
    "3. RegressionEvaluator is used evaluate the regression model during hyperparameter tuning. <br>\n",
    "    3. a. For a two-class classification model use BinaryClassificationEvaluator. <br>\n",
    "    3. b. For multiclass classification use MulticlassClassificationEvaluator or MultilabelClassificationEvaluator. <br>\n",
    "    3. c. For a clustering model use ClusteringEvaluator.\n",
    "4. VectorAssembler is a transformer used to merge multiple columns into a vector column.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "toyota.csv <br>\n",
    "\n",
    "1. model: identifies the model of the vehicle <br>\n",
    "2. year: denotes the registration year of the vehicle <br>\n",
    "3. price: denoted the in euros <br>\n",
    "4. transmission: denotes the type of gearbox (i.e., manual and automatic) <br>\n",
    "5. mileage: denotes the distance covered by the vehicle in miles<br>\n",
    "6. fuel type: denotes the of fuel for the vehicle <br>\n",
    "7. tax: denotes the amount of road tax paid for the vehicle <br>\n",
    "8. mpg: denotes the number of miles per gallon of fuel covered by the vehicle <br>\n",
    "9. engine size: denotes engine capacity in number of liters <br>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"toyota.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "View the first 20 rows of the dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+------------+-------+--------+---+----+----------+\n",
      "|model|year|price|transmission|mileage|fuelType|tax| mpg|engineSize|\n",
      "+-----+----+-----+------------+-------+--------+---+----+----------+\n",
      "| GT86|2016|16000|      Manual|  24089|  Petrol|265|36.2|       2.0|\n",
      "| GT86|2017|15995|      Manual|  18615|  Petrol|145|36.2|       2.0|\n",
      "| GT86|2015|13998|      Manual|  27469|  Petrol|265|36.2|       2.0|\n",
      "| GT86|2017|18998|      Manual|  14736|  Petrol|150|36.2|       2.0|\n",
      "| GT86|2017|17498|      Manual|  36284|  Petrol|145|36.2|       2.0|\n",
      "| GT86|2017|15998|      Manual|  26919|  Petrol|260|36.2|       2.0|\n",
      "| GT86|2017|18522|      Manual|  10456|  Petrol|145|36.2|       2.0|\n",
      "| GT86|2017|18995|      Manual|  12340|  Petrol|145|36.2|       2.0|\n",
      "| GT86|2020|27998|      Manual|    516|  Petrol|150|33.2|       2.0|\n",
      "| GT86|2016|13990|      Manual|  37999|  Petrol|265|36.2|       2.0|\n",
      "| GT86|2013|10495|      Manual|  72000|  Petrol|265|36.2|       2.0|\n",
      "| GT86|2017|17990|      Manual|  12597|  Petrol|145|36.2|       2.0|\n",
      "| GT86|2017|16995|      Manual|  36100|  Petrol|145|36.2|       2.0|\n",
      "| GT86|2019|23995|      Manual|    995|  Petrol|145|33.2|       2.0|\n",
      "| GT86|2018|18498|      Manual|  35228|  Petrol|145|36.2|       2.0|\n",
      "| GT86|2019|23980|      Manual|   1751|  Petrol|145|33.2|       2.0|\n",
      "| GT86|2017|17995|      Manual|  16444|  Petrol|265|36.2|       2.0|\n",
      "| GT86|2014|12998|      Manual|  25499|  Petrol|260|36.2|       2.0|\n",
      "| GT86|2019|23495|   Automatic|   3934|  Petrol|145|32.8|       2.0|\n",
      "| GT86|2019|25780|      Manual|   5123|  Petrol|145|33.2|       2.0|\n",
      "+-----+----+-----+------------+-------+--------+---+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "View the schema of the dataframe\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- model: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- mileage: string (nullable = true)\n",
      " |-- fuelType: string (nullable = true)\n",
      " |-- tax: string (nullable = true)\n",
      " |-- mpg: string (nullable = true)\n",
      " |-- engineSize: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine a count of the unique values in each column of the dataframe. To view the unique values, remove the function count() in the below cell.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in each column are \n",
      "\n",
      "model 18\n",
      "year 23\n",
      "price 2114\n",
      "transmission 4\n",
      "mileage 5699\n",
      "fuelType 4\n",
      "tax 29\n",
      "mpg 81\n",
      "engineSize 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in each column are \\n\")\n",
    "for col in df.columns:\n",
    "    print(col, df.select(col).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Check for any missing values in the dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check for Missing values\n",
      "+-----+----+-----+------------+-------+--------+---+---+----------+\n",
      "|model|year|price|transmission|mileage|fuelType|tax|mpg|engineSize|\n",
      "+-----+----+-----+------------+-------+--------+---+---+----------+\n",
      "|    0|   0|    0|           0|      0|       0|  0|  0|         0|\n",
      "+-----+----+-----+------------+-------+--------+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nCheck for Missing values')\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Check for any missing values in the dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check for Null values\n",
      "+-----+----+-----+------------+-------+--------+---+---+----------+\n",
      "|model|year|price|transmission|mileage|fuelType|tax|mpg|engineSize|\n",
      "+-----+----+-----+------------+-------+--------+---+---+----------+\n",
      "|    0|   0|    0|           0|      0|       0|  0|  0|         0|\n",
      "+-----+----+-----+------------+-------+--------+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nCheck for Null values')\n",
    "\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Encode the categorical columns in the dataframe to a numerical value using label indexing. For example, two string values such as 'A' and 'B' will be encoded as '1' and '2'. New columns will be added in the dataframe corresponding to each of the encoded columns.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+------------+-------+--------+---+----+----------+-------+-------+--------+\n",
      "|model|year|price|transmission|mileage|fuelType|tax| mpg|engineSize|m_index|t_index|ft_index|\n",
      "+-----+----+-----+------------+-------+--------+---+----+----------+-------+-------+--------+\n",
      "| GT86|2016|16000|      Manual|  24089|  Petrol|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|15995|      Manual|  18615|  Petrol|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2015|13998|      Manual|  27469|  Petrol|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|18998|      Manual|  14736|  Petrol|150|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|17498|      Manual|  36284|  Petrol|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|15998|      Manual|  26919|  Petrol|260|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|18522|      Manual|  10456|  Petrol|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|18995|      Manual|  12340|  Petrol|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2020|27998|      Manual|    516|  Petrol|150|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2016|13990|      Manual|  37999|  Petrol|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2013|10495|      Manual|  72000|  Petrol|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|17990|      Manual|  12597|  Petrol|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|16995|      Manual|  36100|  Petrol|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2019|23995|      Manual|    995|  Petrol|145|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2018|18498|      Manual|  35228|  Petrol|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2019|23980|      Manual|   1751|  Petrol|145|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2017|17995|      Manual|  16444|  Petrol|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2014|12998|      Manual|  25499|  Petrol|260|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "| GT86|2019|23495|   Automatic|   3934|  Petrol|145|32.8|       2.0|   10.0|    1.0|     0.0|\n",
      "| GT86|2019|25780|      Manual|   5123|  Petrol|145|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "+-----+----+-----+------------+-------+--------+---+----+----------+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexer = [StringIndexer(inputCol='model', outputCol=\"m_index\"), \\\n",
    "    StringIndexer(inputCol='transmission', outputCol=\"t_index\"), \\\n",
    "        StringIndexer(inputCol='fuelType', outputCol=\"ft_index\")]\n",
    "        \n",
    "pipeline = Pipeline(stages=indexer)\n",
    "\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Dropping the columns with the categorical values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+---+----+----------+-------+-------+--------+\n",
      "|year|price|mileage|tax| mpg|engineSize|m_index|t_index|ft_index|\n",
      "+----+-----+-------+---+----+----------+-------+-------+--------+\n",
      "|2016|16000|  24089|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|15995|  18615|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2015|13998|  27469|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|18998|  14736|150|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|17498|  36284|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|15998|  26919|260|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|18522|  10456|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|18995|  12340|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2020|27998|    516|150|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2016|13990|  37999|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2013|10495|  72000|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|17990|  12597|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|16995|  36100|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2019|23995|    995|145|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2018|18498|  35228|145|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2019|23980|   1751|145|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017|17995|  16444|265|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2014|12998|  25499|260|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2019|23495|   3934|145|32.8|       2.0|   10.0|    1.0|     0.0|\n",
      "|2019|25780|   5123|145|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "+----+-----+-------+---+----+----------+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed = df_indexed.drop('model', 'transmission', 'fuelType')\n",
    "df = df_indexed\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Casting all the cloumns from any datatype to 'float' type.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: float (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- mileage: float (nullable = true)\n",
      " |-- tax: float (nullable = true)\n",
      " |-- mpg: float (nullable = true)\n",
      " |-- engineSize: float (nullable = true)\n",
      " |-- m_index: float (nullable = false)\n",
      " |-- t_index: float (nullable = false)\n",
      " |-- ft_index: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col_name in df_indexed.columns:\n",
    "    df_indexed = df_indexed.withColumn(col_name, col(col_name).cast('float'))\n",
    "\n",
    "df_indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-----+----+----------+-------+-------+--------+\n",
      "|  year|  price|mileage|  tax| mpg|engineSize|m_index|t_index|ft_index|\n",
      "+------+-------+-------+-----+----+----------+-------+-------+--------+\n",
      "|2016.0|16000.0|24089.0|265.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|15995.0|18615.0|145.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2015.0|13998.0|27469.0|265.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|18998.0|14736.0|150.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|17498.0|36284.0|145.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|15998.0|26919.0|260.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|18522.0|10456.0|145.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|18995.0|12340.0|145.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2020.0|27998.0|  516.0|150.0|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2016.0|13990.0|37999.0|265.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2013.0|10495.0|72000.0|265.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|17990.0|12597.0|145.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|16995.0|36100.0|145.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2019.0|23995.0|  995.0|145.0|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2018.0|18498.0|35228.0|145.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2019.0|23980.0| 1751.0|145.0|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2017.0|17995.0|16444.0|265.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2014.0|12998.0|25499.0|260.0|36.2|       2.0|   10.0|    0.0|     0.0|\n",
      "|2019.0|23495.0| 3934.0|145.0|32.8|       2.0|   10.0|    1.0|     0.0|\n",
      "|2019.0|25780.0| 5123.0|145.0|33.2|       2.0|   10.0|    0.0|     0.0|\n",
      "+------+-------+-------+-----+----+----------+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Determine if the duplicate rows are present or not. <br>\n",
    "2. Drop or delete the duplicated rows in the dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Data before dropping duplicates is  6738 9\n",
      "Number of distinct columns are  6699\n",
      "Size of Data after dropping duplicates is  6699 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Data before dropping duplicates is \", df.count(), len(df.columns))\n",
    "\n",
    "print(\"Number of distinct columns are \", df.distinct().count())\n",
    "df = df.dropDuplicates()\n",
    "df_indexed = df_indexed.dropDuplicates()\n",
    "\n",
    "print(\"Size of Data after dropping duplicates is \",df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine the statistical attributes of the columns in the dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 91:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "|summary|              year|            price|           mileage|              tax|               mpg|        engineSize|          m_index|           t_index|           ft_index|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "|  count|              6699|             6699|              6699|             6699|              6699|              6699|             6699|              6699|               6699|\n",
      "|   mean|2016.7427974324526|12529.79907448873|22889.588744588746| 94.5499328257949|63.078728168383925| 1.471995820271686|2.070607553366174|0.4720107478728168|0.49962680997163755|\n",
      "| stddev|2.2052707016915356|6358.562625250668|19109.288501160918|73.94264929532783| 15.86103713724215|0.4356237511150278|2.623128024157879|0.5709580847588824| 0.7015689735428318|\n",
      "|    min|              1998|            10000|                10|                0|             134.5|               0.0|              0.0|               0.0|                0.0|\n",
      "|    25%|            2016.0|           8260.0|            9484.0|              0.0|              55.4|               1.0|              0.0|               0.0|                0.0|\n",
      "|    50%|            2017.0|          10795.0|           18569.0|            135.0|              62.8|               1.5|              1.0|               0.0|                0.0|\n",
      "|    75%|            2018.0|          14995.0|           31059.0|            145.0|              70.6|               1.8|              3.0|               1.0|                1.0|\n",
      "|    max|              2020|             9999|              9999|              565|              94.1|               4.5|             17.0|               3.0|                3.0|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine the Pearson correlation matrix for the columns of the Dataframe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/21 10:09:37 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/08/21 10:09:37 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "/opt/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=df_indexed.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df_indexed).select(vector_col)\n",
    "\n",
    "matrix = Correlation.corr(df_vector, vector_col).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Display the row of the Pearson correlation matrix corresponding to the row of 'price' (i.e., label column)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "[ 0.42281277  1.         -0.30059762  0.21540119 -0.03968017  0.72879115\n",
      "  0.59063058  0.48990246  0.44138194]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pearson correlation matrix:\\n\" + str(matrix[0].toArray()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Select the top three features that are highly correlated with the label column (i.e., 'price'), and drop the remaining columns.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+-------+\n",
      "|  price|engineSize|m_index|t_index|\n",
      "+-------+----------+-------+-------+\n",
      "|15790.0|       1.2|    5.0|    0.0|\n",
      "|27570.0|       2.5|    4.0|    1.0|\n",
      "|18995.0|       2.5|    4.0|    1.0|\n",
      "|10311.0|       2.2|    4.0|    0.0|\n",
      "|14795.0|       2.0|    4.0|    0.0|\n",
      "+-------+----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = df_indexed.drop('year','mileage', 'tax', 'mpg', 'ft_index') \n",
    "df_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Apply Min-Max normalization technique to the selected features.\n",
    "2. Create a vector of features using Vector Assembler.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+-------+--------------------+--------------------+\n",
      "|  price|engineSize|m_index|t_index|            features|     scaled_features|\n",
      "+-------+----------+-------+-------+--------------------+--------------------+\n",
      "|15790.0|       1.2|    5.0|    0.0|[1.20000004768371...|[0.26666667726304...|\n",
      "|27570.0|       2.5|    4.0|    1.0|       [2.5,4.0,1.0]|[0.55555555555555...|\n",
      "|18995.0|       2.5|    4.0|    1.0|       [2.5,4.0,1.0]|[0.55555555555555...|\n",
      "|10311.0|       2.2|    4.0|    0.0|[2.20000004768371...|[0.48888889948527...|\n",
      "|14795.0|       2.0|    4.0|    0.0|       [2.0,4.0,0.0]|[0.44444444444444...|\n",
      "+-------+----------+-------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['engineSize','m_index','t_index'], outputCol='features')\n",
    "df_input = assembler.transform(df_input)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "df_input = scaler.fit(df_input).transform(df_input)\n",
    "df_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Apply Min-Max normalization on the 'price' column.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+-------+--------------------+--------------------+---------+--------------------+\n",
      "|  price|engineSize|m_index|t_index|            features|     scaled_features|    label|        scaled_price|\n",
      "+-------+----------+-------+-------+--------------------+--------------------+---------+--------------------+\n",
      "|15790.0|       1.2|    5.0|    0.0|[1.20000004768371...|[0.26666667726304...|[15790.0]|[0.2525995434948009]|\n",
      "|27570.0|       2.5|    4.0|    1.0|       [2.5,4.0,1.0]|[0.55555555555555...|[27570.0]|[0.4517710710964578]|\n",
      "|18995.0|       2.5|    4.0|    1.0|       [2.5,4.0,1.0]|[0.55555555555555...|[18995.0]|[0.30678840138642...|\n",
      "|10311.0|       2.2|    4.0|    0.0|[2.20000004768371...|[0.48888889948527...|[10311.0]|[0.15996280328007...|\n",
      "|14795.0|       2.0|    4.0|    0.0|       [2.0,4.0,0.0]|[0.44444444444444...|[14795.0]|[0.23577648152844...|\n",
      "+-------+----------+-------+-------+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['price'], outputCol='label')\n",
    "df_input = assembler.transform(df_input)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"label\", outputCol=\"scaled_price\")\n",
    "df_input = scaler.fit(df_input).transform(df_input)\n",
    "df_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The label column ('price') should be of type 'float' or 'double' for providing it to the regression model. <br>\n",
    "Since the Min-Max normalization generates the output as a vector. <br>\n",
    "We convert the label column to a data type of 'float'\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- scaled_features: vector (nullable = true)\n",
      " |-- scaled_price: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 127:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|     scaled_features|scaled_price|\n",
      "+--------------------+------------+\n",
      "|[0.26666667726304...|  0.25259954|\n",
      "|[0.55555555555555...|  0.45177108|\n",
      "|[0.55555555555555...|   0.3067884|\n",
      "|[0.48888889948527...|   0.1599628|\n",
      "|[0.44444444444444...|  0.23577648|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- scaled_features: vector (nullable = true)\n",
      " |-- scaled_price: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "#from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "df_data = df_input.select('scaled_features','scaled_price')\n",
    "df_data.printSchema()\n",
    "\n",
    "udf1 = F.udf(lambda x : float(x[0]),FloatType())\n",
    "df_data = df_input.withColumn('scaled_price',udf1('scaled_price').alias('scaled_price_')).select('scaled_features','scaled_price')\n",
    "df_data.show(5)\n",
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We divide the Data-set into Training and Testing parts in a ratio of 70-30.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_data.randomSplit([0.7, 0.3], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+\n",
      "|scaled_features               |scaled_price|\n",
      "+------------------------------+------------+\n",
      "|[0.0,0.058823529411764705,0.0]|0.12088934  |\n",
      "|[0.0,0.058823529411764705,0.0]|0.12088934  |\n",
      "|[0.0,0.1764705882352941,0.0]  |0.22740722  |\n",
      "|[0.2222222222222222,0.0,0.0]  |0.00211345  |\n",
      "|[0.2222222222222222,0.0,0.0]  |0.01183532  |\n",
      "+------------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 130:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.show(n = 5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Configure a Linear Regression Model by specifying the input column, output column, and the maximum number of iterations. <br>\n",
    "2. Define the Parameter Grid for the Linear Regression Model.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol='scaled_price', maxIter=100)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0.2, 0.8]) \\\n",
    "    .addGrid(lr.regParam, [0.3]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Use the CrossValidator for hyperparmeter tuning for the Linear Regression model using the defined parameter grid.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"scaled_price\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=2,\n",
    "                          parallelism=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Train the Linear regression model with crossvalidator\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Obtain the best model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel: uid=LinearRegression_597882899058, numFeatures=3\n"
     ]
    }
   ],
   "source": [
    "print(cvModel.bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Display the details of all the models trained \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.10215530060177624,\n",
       "  {Param(parent='LinearRegression_597882899058', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "   Param(parent='LinearRegression_597882899058', name='regParam', doc='regularization parameter (>= 0).'): 0.3}),\n",
       " (0.10571605653888813,\n",
       "  {Param(parent='LinearRegression_597882899058', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.8,\n",
       "   Param(parent='LinearRegression_597882899058', name='regParam', doc='regularization parameter (>= 0).'): 0.3})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cvModel.avgMetrics, cvModel.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine the Mean squared error of the best model on the test data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_summary = cvModel.bestModel.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1046462215018284\n"
     ]
    }
   ],
   "source": [
    "print(e_summary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Determine the predictions of the best model on the test data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = cvModel.bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+\n",
      "|     scaled_features|scaled_price|         prediction|\n",
      "+--------------------+------------+-------------------+\n",
      "|           (3,[],[])|  0.19359203| 0.1786395857079632|\n",
      "|[0.0,0.0,0.333333...|  0.17161214| 0.1786395857079632|\n",
      "|[0.0,0.0588235294...|  0.15132302|0.17867289303133807|\n",
      "|[0.22222222222222...|  0.07515428| 0.1914056336096327|\n",
      "|[0.22222222222222...|  0.08196805| 0.1914056336096327|\n",
      "|[0.22222222222222...|  0.08536647| 0.1914056336096327|\n",
      "|[0.22222222222222...| 0.086735986| 0.1914056336096327|\n",
      "|[0.22222222222222...|  0.08690506| 0.1914056336096327|\n",
      "|[0.22222222222222...|  0.08842675| 0.1914056336096327|\n",
      "|[0.22222222222222...|  0.09197734| 0.1914056336096327|\n",
      "+--------------------+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the hyperparmeter tuning process for the Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Configure the Random Forest Regressor <br>\n",
    "2. Define Parameter Grid <br>\n",
    "3. Use the Cross Validator for hyperparameter tuning <br>\n",
    "4. Determine the best model <br>\n",
    "5. Display the details of all the models trained <br>\n",
    "6. Evaluate the best model on the test data and display the MSE <br>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol='scaled_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [1, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"scaled_price\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=2,\n",
    "                          parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressionModel: uid=RandomForestRegressor_2deb64e860a4, numTrees=1, numFeatures=3\n"
     ]
    }
   ],
   "source": [
    "print(cvModel.bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.04971509428578412,\n",
       "  {Param(parent='RandomForestRegressor_2deb64e860a4', name='numTrees', doc='Number of trees to train (>= 1).'): 1,\n",
       "   Param(parent='RandomForestRegressor_2deb64e860a4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}),\n",
       " (0.05028101498389054,\n",
       "  {Param(parent='RandomForestRegressor_2deb64e860a4', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       "   Param(parent='RandomForestRegressor_2deb64e860a4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cvModel.avgMetrics, cvModel.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = cvModel.bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+\n",
      "|     scaled_features|scaled_price|         prediction|\n",
      "+--------------------+------------+-------------------+\n",
      "|           (3,[],[])|  0.19359203|0.11404406423432714|\n",
      "|[0.0,0.0,0.333333...|  0.17161214|0.14120883116789903|\n",
      "|[0.0,0.0588235294...|  0.15132302|0.11404406423432714|\n",
      "|[0.22222222222222...|  0.07515428|0.11404406423432714|\n",
      "|[0.22222222222222...|  0.08196805|0.11404406423432714|\n",
      "|[0.22222222222222...|  0.08536647|0.11404406423432714|\n",
      "|[0.22222222222222...| 0.086735986|0.11404406423432714|\n",
      "|[0.22222222222222...|  0.08690506|0.11404406423432714|\n",
      "|[0.22222222222222...|  0.08842675|0.11404406423432714|\n",
      "|[0.22222222222222...|  0.09197734|0.11404406423432714|\n",
      "+--------------------+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"scaled_price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0526963909254728\n"
     ]
    }
   ],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "485ad2d47d78171392804dc4ab56d68e22869fca1387c3a77d012fec7abc62c2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
